{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c429bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset-news\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "title = []\n",
    "articles = []\n",
    "topics = ['auto', 'baseball', 'electronics', 'hockey', 'ibm-hw', 'moto', 'pol-guns', 'mac-hw']\n",
    "path = r'dataset-news'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52792d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in listdir(path):\n",
    "    for j in topics:\n",
    "        if i.startswith(j): #是否以topic中的单词开头\n",
    "            title.append(j)\n",
    "            file = open(join(path,i), 'r') #join用于拼接文件路径（指向文件名）\n",
    "            articles.append(file.read())\n",
    "            file.close()\n",
    "#len(title)\n",
    "#len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6678c1e",
   "metadata": {},
   "source": [
    "#### 1. turn each document into a vector in the Euclidean space. Each dimension of the vector corresponds to a word, with its value specifying the number of occurrences of the corresponding word in the corresponding document. This can be done by using CountVectorizer in scikit-learn (see tutorial). It helps to remove so called “stopwords”, which are words that occur often in the English language such as articles (e.g. ’the’,’a’,), adjectives (e.g. ’my’,’yours) etc. Stopwords usually do not carry much information about the topic of a document and should be removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5612adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vector = CountVectorizer(stop_words = 'english') #去除常见英文单词\n",
    "text_vector = count_vector.fit_transform(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03b4da02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 14551)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e4c8b",
   "metadata": {},
   "source": [
    "#### 2. train a naive Bayes classifier on the collection of documents. In Python, we are going to consider the multinomial naive Bayes classifier and the Gaussian naive Bayes classifier. See tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b1145e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points in Gaussian Naive Bayes algorithm out of a total 800 points : 27\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(text_vector.toarray(),title)\n",
    "\n",
    "y_pred = gnb.predict(text_vector.toarray())\n",
    "print (\"Number of mislabeled points in Gaussian Naive Bayes algorithm out of a total %d points : %d\" % \\\n",
    "(text_vector.shape[0],(title != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc30607e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points in Multinominal Naive Bayes algorithm out of a total 800 points : 32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB(alpha=1.0)\n",
    "mnb.fit(text_vector, title)\n",
    "\n",
    "y_pred2 = mnb.predict(text_vector)\n",
    "print (\"Number of mislabeled points in Multinominal Naive Bayes algorithm out of a total %d points : %d\" % \\\n",
    "(text_vector.shape[0],(target != y_pred2).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac11d62",
   "metadata": {},
   "source": [
    "#### 3. perform a 10-fold cross validation to determine which classifier performs best. More informations on how to do that are provided in the tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce85870",
   "metadata": {},
   "source": [
    "#### Task a: perform a 10-fold cross validation and report the average accuracy (i.e. average number of documents classified correctly) for both the multinomial and Gaussian naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5544a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "177d2cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB classifier has the average accuracy of 0.71875\n"
     ]
    }
   ],
   "source": [
    "#Gaussian Naive Bayes classifier\n",
    "\n",
    "counter = 0\n",
    "accuracy = []\n",
    "\n",
    "for train, test in kf.split(text_vector):\n",
    "    train_split1 = text_vector.toarray()[train]\n",
    "    train_split2 = np.array(title)[train]\n",
    "    test_split1 = text_vector.toarray()[test]\n",
    "    test_split2 = np.array(title)[test]\n",
    "    counter += 1\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(train_split1, train_split2)\n",
    "    \n",
    "    accuracy.append(gnb.score(test_split1, test_split2))\n",
    "    \n",
    "print ('Gaussian NB classifier has the average accuracy of {}'.format(sum(accuracy)/len(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52cbc5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinominal NB classifier has the average accuracy of 0.7875\n"
     ]
    }
   ],
   "source": [
    "#Multinominal NB classifier\n",
    "\n",
    "counter1 = 0\n",
    "accuracy1 = []\n",
    "\n",
    "for train, test in kf.split(text_vector):\n",
    "    train_split1 = text_vector.toarray()[train]\n",
    "    train_split2 = np.array(title)[train]\n",
    "    test_split1 = text_vector.toarray()[test]\n",
    "    test_split2 = np.array(title)[test]\n",
    "    \n",
    "    counter1 += 1\n",
    "    mnb = MultinomialNB(alpha=1.0)\n",
    "    mnb.fit(train_split1, train_split2)\n",
    "    \n",
    "    accuracy1.append(mnb.score(test_split1, test_split2))\n",
    "    \n",
    "print ('Multinominal NB classifier has the average accuracy of {}'.format(sum(accuracy1)/len(accuracy1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccee70f",
   "metadata": {},
   "source": [
    "#### Task b: consider the best classifier according to your evaluation. How does such a classifier compare with a random classifier which classifies the documents randomly? (i.e. it assigns to each document one of the eight topics, chosen randomly). To answer this question, compute the accuracy of the random classifier (i.e. the probability that a document is classified correctly) and include it in your Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88e1abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate accuracy of random classifier\n",
    "\n",
    "import random\n",
    "random_vector = []\n",
    "for i in range(len(title)):\n",
    "    random_vector.append(random.choice(topics))\n",
    "    \n",
    "#random_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57768e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random classifier has an accuracy score of 0.13625.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Random classifier has an accuracy score of {}.\".format(accuracy_score(title, random_vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db14d66",
   "metadata": {},
   "source": [
    "The accuracy score of a random classifier is far below the Gaussian NB classifier and the Multinominal NB classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8381fe",
   "metadata": {},
   "source": [
    "#### Task c: report the accuracy of the best classifier when the “stopwords” are not removed. Does the accuracy improve or worsen? Try to explain why the accuracy improves or worsens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7e1445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector_none = CountVectorizer(stop_words = None) #不去除常见英文单词\n",
    "text_vector_none = count_vector_none.fit_transform(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb5a5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2e524c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Stopwords are not removed) Multinominal NB classifier has the average accuracy of 0.72875\n"
     ]
    }
   ],
   "source": [
    "#Multinominal NB classifier\n",
    "accuracy3 = []\n",
    "counter3 = 0\n",
    "\n",
    "for train, test in kf.split(text_vector_none):\n",
    "    train_split1 = text_vector_none.toarray()[train]\n",
    "    train_split2 = np.array(title)[train]\n",
    "    test_split1 = text_vector_none.toarray()[test]\n",
    "    test_split2 = np.array(title)[test]\n",
    "    \n",
    "    counter3 += 1\n",
    "    mnb = MultinomialNB(alpha=1.0)\n",
    "    mnb.fit(train_split1, train_split2)\n",
    "    \n",
    "    accuracy3.append(mnb.score(test_split1, test_split2))\n",
    "    \n",
    "print ('(Stopwords are not removed) Multinominal NB classifier has the average accuracy of {}'.format(sum(accuracy3)/len(accuracy3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c852906",
   "metadata": {},
   "source": [
    "The accuracy for Multinominal NB classifier for text vectors that removed stopwords is 0.7875, which is slightly higher. \n",
    "It is possible that the presence of stopwords can lead to overfitting of the classifier to the relationship between them and the title. This is because stopwords may not carry useful information and may increase noise in the data, which can negatively affect the performance of the classifier. Therefore, removing stopwords can help improve the accuracy and generalization ability of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641254ef",
   "metadata": {},
   "source": [
    "#### Task d: Consider the following two classification tasks. In the first task, you should consider only the documents related to the following topics: “use of guns”, “hockey”,“Mac hardware”, while in the second task you should consider “Mac hardware”,“IBM hardware” and “electronics”. Perform a 10-fold cross validation for each of the two tasks and report the average accuracy. Try to explain why one task might be easier than the other one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29adde",
   "metadata": {},
   "source": [
    "#### Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30058b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = [i for i in range(len(title)) if title[i] in ([\"pol-guns\", \"hockey\", \"mac-hw\"])]\n",
    "task1_vector = text_vector.toarray()[index1]\n",
    "title_task1 = np.array(title)[index1]\n",
    "kf = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "537f2676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB classifier for Task1 has the average accuracy of 0.9433333333333334\n"
     ]
    }
   ],
   "source": [
    "accuracy4 = []\n",
    "counter4 = 0\n",
    "\n",
    "for train, test in kf.split(task1_vector):\n",
    "    train_split1 = task1_vector[train]\n",
    "    train_split2 = title_task1[train]\n",
    "    test_split1 = task1_vector[test]\n",
    "    test_split2 = title_task1[test]\n",
    "    \n",
    "    counter4 += 1\n",
    "    mnb = MultinomialNB(alpha=1.0)\n",
    "    mnb.fit(train_split1, train_split2)\n",
    "    \n",
    "    accuracy4.append(mnb.score(test_split1, test_split2))\n",
    "    \n",
    "print ('MNB classifier for Task1 has the average accuracy of {}'.format(sum(accuracy4)/len(accuracy4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ddad7",
   "metadata": {},
   "source": [
    "#### Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da8a6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2 = [i for i in range(len(title)) if title[i] in ([\"electronics\", \"ibm-hw\", \"mac-hw\"])]\n",
    "task2_vector = text_vector.toarray()[index2]\n",
    "title_task2 = np.array(title)[index2]\n",
    "kf = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b81a5bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB classifier for Task2 has the average accuracy of 0.7633333333333334\n"
     ]
    }
   ],
   "source": [
    "accuracy5 = []\n",
    "counter5 = 0\n",
    "\n",
    "for train, test in kf.split(task2_vector):\n",
    "    train_split1 = task2_vector[train]\n",
    "    train_split2 = title_task2[train]\n",
    "    test_split1 = task2_vector[test]\n",
    "    test_split2 = title_task2[test]\n",
    "    \n",
    "    counter5 += 1\n",
    "    mnb = MultinomialNB(alpha=1.0)\n",
    "    mnb.fit(train_split1, train_split2)\n",
    "    \n",
    "    accuracy5.append(mnb.score(test_split1, test_split2))\n",
    "    \n",
    "print ('MNB classifier for Task2 has the average accuracy of {}'.format(sum(accuracy5)/len(accuracy5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da57cf",
   "metadata": {},
   "source": [
    "The accuracy for Task 1 is 0.94 and the accuracy for Task 2 is 0.76. We can find that Task 1 is easier than Task 2. \n",
    "The decrease in accuracy without removing stopwords may be attributed to the significant variations in text among classes in task 1 (\"use of guns\", \"hockey\", \"Mac hardware\") compared to the relatively smaller differences in text among classes in task 2 (\"Mac hardware\", \"IBM hardware\", and \"electronics\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ebd885",
   "metadata": {},
   "source": [
    "In text classification tasks, the performance of a classifier may be inferior for a group of articles with similar topics compared to a group of articles with diverse topics. This is because articles with similar topics often contain similar vocabulary and phrases, which results in similar word frequencies across different categories. This makes it challenging for the classifier to differentiate key features between different categories, leading to an increase in classification errors. Conversely, for a group of articles with diverse topics, the classifier can more easily identify differences between different categories, resulting in better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf21b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
